{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beijing_air_plollution_2(final).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1E0HnMgV9uGmuyiDfhruLFpt_oiac2gq8",
      "authorship_tag": "ABX9TyNgKXXYq/ren1v3dBIwjIQ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YMGYM/TSE_Learning/blob/master/Beijing_air_plollution_2(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPythiWfQCnX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "이 파일은 황철현, 신강욱의\n",
        "`미세먼지 예측 성능 개선을 위한 CNN-LSTM 결합 방법`\n",
        "논문의 구현 연습 파일입니다.\n",
        "\n",
        "데이터셋은 [Beijing PM2.5 데이터셋](https://www.kaggle.com/djhavera/beijing-pm25-data-data-set)\n",
        "을 사용했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMnquIS4QfaO",
        "colab_type": "text"
      },
      "source": [
        "# Import All"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ9ZCMavQg_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGdslSrQXBM",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xY3TOYzQbgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b5bb416-55c7-4023-850b-f5d17de4ddba"
      },
      "source": [
        "! unzip /content/drive/My\\ Drive/Datasets/beijing_air.zip -d data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Datasets/beijing_air.zip\n",
            "  inflating: data/PRSA_data_2010.1.1-2014.12.31.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5W7oRhQ6Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "  all_data = pd.read_csv('/content/data/PRSA_data_2010.1.1-2014.12.31.csv') # 전체 데이터\n",
        "  dropped_data = all_data.drop(['No', 'year', 'month', 'day', 'hour'],axis=1) # 필요 없는 데이터는 버림\n",
        "  pm25 = dropped_data.pop('pm2.5') # 미세먼지 데이터 확인\n",
        "\n",
        "  return pm25, dropped_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeP7jXsgQ4Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pm25, proxy = get_data()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o932fEU9RP3E",
        "colab_type": "text"
      },
      "source": [
        "# NaN Data fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw7U7JGkRSoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pm25 = pm25.fillna(method='pad')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5dnAMuOR1N0",
        "colab_type": "text"
      },
      "source": [
        "# Make Normalize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhbeIxNDR1hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PmScaler:\n",
        "  def __init__(self):\n",
        "    self.scaler = MinMaxScaler()\n",
        "  \n",
        "  def make_norlized_dataset(self, x, rate): \n",
        "    arrlen = int(len(x) * (rate))\n",
        "    if isinstance(x, type(np.array([]))) == False:\n",
        "      reshaped = x.to_numpy().reshape(-1,1)\n",
        "    else:\n",
        "      reshaped = x.reshape(-1,1)\n",
        "    scaled_data = self.scaler.fit_transform(reshaped)\n",
        "\n",
        "    train, val, test = scaled_data[:-1 * (arrlen * 2)], scaled_data[-1 * (arrlen * 2) : -1 * (arrlen)], scaled_data[-1 * (arrlen):]\n",
        "\n",
        "    return train, val, test\n",
        "  \n",
        "  def invert_scale(self, x):\n",
        "    inverse = self.scaler.inverse_transform(x)\n",
        "    return inverse"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_v2XZOFR9yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = PmScaler()\n",
        "lstm_x_train, lstm_x_val, lstm_x_test = scaler.make_norlized_dataset(pm25, 0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iO5V6OhRZNJ",
        "colab_type": "text"
      },
      "source": [
        "# Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arSPbMdqRepE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EntireModel():\n",
        "  def __init__(self, pm25, proxydata, pmScaler, proxyScaler):\n",
        "    self.cnn_model = _get_cnn_model()\n",
        "    self.lstm_model = _get_lstm_model()\n",
        "    self.pm25 = pm25\n",
        "    self.proxydata = proxydata\n",
        "    self.lstm_train_data_gen = K.preprocessing.sequence.TimeseriesGenerator(lstm_x_train, lstm_x_train, length=15, shuffle=True)\n",
        "    self.lstm_val_data_gen = K.preprocessing.sequence.TimeseriesGenerator(lstm_x_val, lstm_x_val, length=15, shuffle=True)\n",
        "    self.lstm_test_data_gen = K.preprocessing.sequence.TimeseriesGenerator(lstm_x_test, lstm_x_test, length=15, batch_size = 1, shuffle=False)\n",
        "    self.lstm_callbacks = [K.callbacks.TensorBoard(log_dir='lstm_logs')]\n",
        "    self.cnn_callbacks = [K.callbacks.TensorBoard(log_dir='cnn_logs')]\n",
        "\n",
        "  def cnn_model_fit(self epochs=1):\n",
        "    ...\n",
        "    self.cnn_model.fit()\n",
        "\n",
        "  def lstm_model_fit(self, epochs=1):\n",
        "\n",
        "    self.lstm_model.fit()\n",
        "\n",
        "\n",
        "  def _get_cnn_model(self):\n",
        "    cnnModel = K.Sequential()\n",
        "    cnnModel.add(K.layers.Conv2DTranspose(32, (2,2), input_shape=(1,x_train.shape[1],x_train.shape[2]), activation=\"relu\"))\n",
        "    cnnModel.add(K.layers.MaxPool2D(strides=2))\n",
        "    cnnModel.add(K.layers.Flatten())\n",
        "    cnnModel.add(K.layers.Dropout(0.1))\n",
        "    cnnModel.add(K.layers.Dense(100, activation=\"relu\"))\n",
        "    cnnModel.add(K.layers.ReLU())\n",
        "    cnnModel.add(K.layers.Dense(5, activation=\"softmax\"))\n",
        "    cnnModel.summary()\n",
        "    cnnModel.compile(optimizer=\"adam\", loss=\"MSE\")\n",
        "\n",
        "    return cnnModel\n",
        "\n",
        "  def _get_lstm_model(self):\n",
        "    lstm_model = K.Sequential()\n",
        "    lstm_model.add(K.layers.LSTM(216, input_shape=(16,1)))\n",
        "    lstm_model.add(K.layers.Dropout(0.3))\n",
        "    lstm_model.add(K.layers.Dense(128, activation=\"relu\"))\n",
        "    lstm_model.add(K.layers.Dropout(0.3))\n",
        "    lstm_model.add(K.layers.Dense(1, activation=\"sigmoid\"))\n",
        "    lstm_model.summary()\n",
        "    lstm_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}