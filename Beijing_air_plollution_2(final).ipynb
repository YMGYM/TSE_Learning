{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beijing_air_plollution_2(final)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1E0HnMgV9uGmuyiDfhruLFpt_oiac2gq8",
      "authorship_tag": "ABX9TyPbGql+8W5NiFJzqDpBfFP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YMGYM/TSE_Learning/blob/master/Beijing_air_plollution_2(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPythiWfQCnX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "이 파일은 황철현, 신강욱의\n",
        "`미세먼지 예측 성능 개선을 위한 CNN-LSTM 결합 방법`\n",
        "논문의 구현 연습 파일입니다.\n",
        "\n",
        "데이터셋은 [Beijing PM2.5 데이터셋](https://www.kaggle.com/djhavera/beijing-pm25-data-data-set)\n",
        "을 사용했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMnquIS4QfaO",
        "colab_type": "text"
      },
      "source": [
        "# Import All"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ9ZCMavQg_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGdslSrQXBM",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xY3TOYzQbgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e33565c2-bcfe-44be-dfaa-faa71a4f3dd9"
      },
      "source": [
        "! unzip /content/drive/My\\ Drive/Datasets/beijing_air.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Datasets/beijing_air.zip\n",
            "  inflating: data/PRSA_data_2010.1.1-2014.12.31.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5W7oRhQ6Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "  all_data = pd.read_csv('/content/data/PRSA_data_2010.1.1-2014.12.31.csv') # 전체 데이터\n",
        "  dropped_data = all_data.drop(['No', 'year', 'month', 'day', 'hour'],axis=1) # 필요 없는 데이터는 버림\n",
        "  pm25 = dropped_data.pop('pm2.5') # 미세먼지 데이터 확인\n",
        "\n",
        "  return pm25, dropped_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeP7jXsgQ4Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pm25, proxy = get_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o932fEU9RP3E",
        "colab_type": "text"
      },
      "source": [
        "# NaN Data fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw7U7JGkRSoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pm25 = pm25.fillna(method='pad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5dnAMuOR1N0",
        "colab_type": "text"
      },
      "source": [
        "# Make Normalize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhbeIxNDR1hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PmScaler:\n",
        "  def __init__(self):\n",
        "    self.scaler = MinMaxScaler()\n",
        "  \n",
        "  def make_norlized_dataset(self, x, rate): \n",
        "    arrlen = int(len(x) * (rate))\n",
        "    if isinstance(x, type(np.array([]))) == False:\n",
        "      reshaped = x.to_numpy().reshape(-1,1)\n",
        "    else:\n",
        "      reshaped = x.reshape(-1,1)\n",
        "    scaled_data = self.scaler.fit_transform(reshaped)\n",
        "\n",
        "    train, val, test = scaled_data[:-1 * (arrlen * 2)], scaled_data[-1 * (arrlen * 2) : -1 * (arrlen)], scaled_data[-1 * (arrlen):]\n",
        "\n",
        "    return train, val, test\n",
        "  \n",
        "  def invert_scale(self, x):\n",
        "    inverse = self.scaler.inverse_transform(x)\n",
        "    return inverse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_v2XZOFR9yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = PmScaler()\n",
        "lstm_x_train, lstm_x_val, lstm_x_test = scaler.make_norlized_dataset(pm25, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1ktmAkpauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProxyDataScaler():\n",
        "  def __init__(self, data):\n",
        "    self.table = data\n",
        "    self.scaler = MinMaxScaler()\n",
        "    \n",
        "  def change_cbwd_data(self):\n",
        "    mapping = {}\n",
        "    cols = self.table[\"cbwd\"].value_counts().index\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "      mapping[col] = i # mapping = {\"SE\" : 0, \"NW\": 1, \"cv\": 2, \"NE\":3}\n",
        "    self.table = self.table.replace({'cbwd' : mapping})\n",
        "    print(\"cbwd data changed to number : {SE : 0, NW: 1, cv: 2, NE:3} \")\n",
        "\n",
        "  def make_normalize_data(self):\n",
        "    self.norm_data = self.scaler.fit_transform(self.table)\n",
        "    return self.norm_data\n",
        "\n",
        "  def slice_proxy_data(self, time):\n",
        "    col_cnt = len(self.table.columns)\n",
        "    if (self.norm_data is not None): # 정규화된 데이터가 있는지 확인\n",
        "      print(\"norm_data detected\")\n",
        "\n",
        "      if isinstance(self.norm_data, type(np.array([]))) == False: # 데이터를 numpy 형식으로 변환\n",
        "        data = self.norm_data.to_numpy().astype(\"float32\")\n",
        "      else:\n",
        "        data = self.norm_data.astype(\"float32\")\n",
        "\n",
        "    else:\n",
        "      print(\"norm_data not detected\")\n",
        "      data = self.table.to_numpy().astype(\"float32\")\n",
        "\n",
        "    self.sliced_data = np.zeros(shape=(1,time,col_cnt))\n",
        "\n",
        "    for i in range((len(data)-time) + 1):\n",
        "      if i == 0:\n",
        "        self.sliced_data = data[:i+time].reshape(1, time,-1)\n",
        "      else:\n",
        "        self.sliced_data = np.vstack((self.sliced_data, data[i:i+time].reshape(1,time,-1)))\n",
        "    # self.sliced_data = self.sliced_data.transpose(0,2,1)\n",
        "    return self.sliced_data\n",
        "\n",
        "  def split_data(self, data = None, rate = 0.1):\n",
        "    if data is None:\n",
        "      arrlen = int(len(self.sliced_data) * (rate))\n",
        "    else:\n",
        "      arrlen = int(len(data) * rate)\n",
        "\n",
        "    data = self.sliced_data\n",
        "    train, val, test = data[:-1 * (arrlen * 2)], data[-1 * (arrlen * 2) : -1 * (arrlen)], data[-1 * (arrlen):]\n",
        "\n",
        "    return train, val, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COffyY3ow6By",
        "colab_type": "text"
      },
      "source": [
        "# New LSTM Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weEk_Ih-w89V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMInputGenerator(K.utils.Sequence):\n",
        "  def __init__(self, lstm_x, lstm_y, data_len, cnn_output):\n",
        "    self.lstm_data_gen = K.preprocessing.sequence.TimeseriesGenerator(lstm_x, lstm_x, batch_size=1, length=data_len, shuffle=False)\n",
        "    self.cnn_output = cnn_output\n",
        "  def __getitem__(self, index):\n",
        "    lstm_x, lstm_y = self.lstm_data_gen[index]\n",
        "    stack_data = self.cnn_output[index].reshape(1,-1,1)\n",
        "    return_x = np.hstack((lstm_x, stack_data))\n",
        "    return return_x, lstm_y\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.lstm_data_gen.__len__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iO5V6OhRZNJ",
        "colab_type": "text"
      },
      "source": [
        "# Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arSPbMdqRepE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "c6c99f3c-159f-4635-cb18-fd9d632c89d8"
      },
      "source": [
        "class EntireModel():\n",
        "  def __init__(self, pm25, proxydata):\n",
        "    # ----------- 모델 생성 --------------\n",
        "    self.cnn_model = _get_cnn_model()\n",
        "    self.lstm_model = _get_lstm_model()\n",
        "\n",
        "    # -----------scaler 클래스 생성 -------------------\n",
        "    self.pm25 = pm25\n",
        "    self.proxydata = proxydata\n",
        "    self.pmScaler = pmScaler()\n",
        "    self.proxyScaler = proxyScaler(proxydata)\n",
        "    \n",
        "    # ---------- train 용 callbacks -----------\n",
        "    self.lstm_callbacks = [K.callbacks.TensorBoard(log_dir='lstm_logs')]\n",
        "    self.cnn_callbacks = [K.callbacks.TensorBoard(log_dir='cnn_logs')]\n",
        "\n",
        "  def make_proxy_data_generator(self, data_len = 2):\n",
        "    # ---------- proxy 데이터 전처리 -----------\n",
        "    self.proxyScaler.change_cbwd_data()\n",
        "    self.proxyScaler.make_normalize_data()\n",
        "    self.proxyScaler.make_normalize_data()\n",
        "    self.proxyScaler.slice_proxy_data(data_len)\n",
        "    cnn_x_train, cnn_x_val, cnn_x_test = self.proxyScaler.split_data()\n",
        "    cnn_y_train, cnn_y_val, cnn_y_test = _cnn_y_split(self.categorical_grad, data_len)\n",
        "\n",
        "    \n",
        "    self.cnn_train_data_gen = K.preprocessing.sequence.TimeseriesGenerator(cnn_x_train, cnn_y_train, length=1, batch_size = 128, shuffle=True)\n",
        "    self.cnn_val_data_gen = K.preprocessing.sequence.TimeseriesGenerator(cnn_x_val, cnn_y_val, length=1, batch_size = 128, shuffle=True)\n",
        "    self.cnn_test_data_gen = K.preprocessing.sequence.TimeseriesGenerator(cnn_x_test, cnn_y_test, length=1, batch_size = 1, shuffle=False)\n",
        "\n",
        "  def make_lstm_data_generator(self, cnn_output, data_len = 15):\n",
        "    # ---------- pm25 데이터 전처리 -----------\n",
        "    pm25_x_train, pm25_x_val, pm25_x_test = pmScaler.make_norlized_dataset(self.pm25, 0.1)\n",
        "\n",
        "    # ---------- pm25 Data Generator -------------\n",
        "    self.pm25_train_data_gen = K.preprocessing.sequence.TimeseriesGenerator(pm25_x_train, pm25_x_train, length=data_len, shuffle=True)\n",
        "    self.pm25_val_data_gen = K.preprocessing.sequence.TimeseriesGenerator(pm25_x_val, pm25_x_val, length=data_len, shuffle=True)\n",
        "    self.pm25_test_data_gen = K.preprocessing.sequence.TimeseriesGenerator(pm25_x_test, pm25_x_test, length=data_len, batch_size = 1, shuffle=False)\n",
        "\n",
        "    # ---------- CNN result ------------\n",
        "    self.cnn_output = self.cnn_model.predict(self.cnn_test_data_gen)\n",
        "    rated_cnn_train_output = _make_lstm_input(self.cnn_output[data_len + 1:], self.pm25_train_data_gen)\n",
        "    rated_cnn_val_output = _make_lstm_input(self.cnn_output[data_len + 1:], self.pm25_val_data_gen)\n",
        "    rated_cnn_test_output = _make_lstm_input(self.cnn_output[data_len + 1:], self.pm25_test_data_gen)\n",
        "\n",
        "    # ---------- LSTM data gen ---------\n",
        "    self.lstm_train_data_gen = LSTMInputGenerator(pm25_x_train, pm25_x_train, data_len, rated_cnn_train_output)\n",
        "    self.lstm_val_data_gen = LSTMInputGenerator(pm25_x_val, pm25_x_val, data_len, rated_cnn_val_output)\n",
        "    self.lstm_test_data_gen = LSTMInputGenerator(pm25_x_test, pm25_x_test, data_len, rated_cnn_test_output)\n",
        "\n",
        "  def cnn_model_fit(self epochs=1):\n",
        "    self.cnn_model.fit(x=self.cnn_train_data_gen, epochs=epochs, validation_data=(self.cnn_val_data_gen), callbacks=self.cnn_callbacks)\n",
        "\n",
        "  def lstm_model_fit(self, epochs=1):\n",
        "    lstm_model.fit(x=self.lstm_train_data_gen, epochs=epochs, validation_data=self.lstm_val_data_gen, callbacks=self.lstm_callbacks)\n",
        "\n",
        "  def total_model_test(self):\n",
        "    return self.lstm_model.predict(self.lstm_test_data_gen)\n",
        "    \n",
        "  def _get_cnn_model(self):\n",
        "    cnnModel = K.Sequential()\n",
        "    cnnModel.add(K.layers.Conv2DTranspose(32, (2,2), input_shape=(1,x_train.shape[1],x_train.shape[2]), activation=\"relu\"))\n",
        "    cnnModel.add(K.layers.MaxPool2D(strides=2))\n",
        "    cnnModel.add(K.layers.Flatten())\n",
        "    cnnModel.add(K.layers.Dropout(0.1))\n",
        "    cnnModel.add(K.layers.Dense(100, activation=\"relu\"))\n",
        "    cnnModel.add(K.layers.ReLU())\n",
        "    cnnModel.add(K.layers.Dense(5, activation=\"softmax\"))\n",
        "    cnnModel.summary()\n",
        "    cnnModel.compile(optimizer=\"adam\", loss=\"MSE\")\n",
        "\n",
        "    return cnnModel\n",
        "\n",
        "  def _get_lstm_model(self):\n",
        "    lstm_model = K.Sequential()\n",
        "    lstm_model.add(K.layers.LSTM(216, input_shape=(16,1)))\n",
        "    lstm_model.add(K.layers.Dropout(0.3))\n",
        "    lstm_model.add(K.layers.Dense(128, activation=\"relu\"))\n",
        "    lstm_model.add(K.layers.Dropout(0.3))\n",
        "    lstm_model.add(K.layers.Dense(1, activation=\"sigmoid\"))\n",
        "    lstm_model.summary()\n",
        "    lstm_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "  def _cnn_y_split(data, data_length, rate=0.1):\n",
        "    data = data[data_length-1:]\n",
        "    data = K.utils.to_categorical(data)\n",
        "    arrlen = int(len(data) * rate)\n",
        "\n",
        "    train, val, test =  data[:-1 * (arrlen * 2)], data[-1 * (arrlen * 2) : -1 * (arrlen)], data[-1 * (arrlen):]\n",
        "    return train, val, test\n",
        "\n",
        "  def _get_grad_pm25(self):\n",
        "    grad_data = self.pm25.pct_change()\n",
        "    grad_data = grad_data.fillna(method=\"pad\")\n",
        "    bins = [-9.166667e-02, -1e-15,1e-15, 1.212121e-01]\n",
        "    grad_level = np.digitize(grad_data, bins=bins, right=False)\n",
        "    self.categorical_grad = K.utils.to_categorical(grad_level)\n",
        "\n",
        "  def _get_rate(data):\n",
        "    index = data.argmax()\n",
        "    if index == 3:\n",
        "      rate = 1\n",
        "    else:\n",
        "      rate = 1 + ((index-3) * 0.25) # 최대 50%의 변화율을 줘 봄\n",
        "\n",
        "    return rate\n",
        "  \n",
        "  def _compute_with_data(data, value):\n",
        "    rate = _get_rate(data)\n",
        "    return rate * value\n",
        "\n",
        "  def _make_lstm_input(cnn_output, lstm_data_gen):\n",
        "    result = np.zeros(shape=(1,))\n",
        "    for i in range(len(cnn_output)):\n",
        "      lx, ly = lstm_data_gen[i]\n",
        "      if i == 0:\n",
        "        result = _compute_with_data(cnn_output[i], ly.squeeze(axis=1))\n",
        "      else:\n",
        "        result = np.hstack((result, compute_with_data(cnn_output[i], ly.squeeze(axis=1))))\n",
        "    return result\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-dedd62e3e39b>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    def cnn_model_fit(self epochs=1):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUOca31BqV8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}