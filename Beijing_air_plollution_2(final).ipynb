{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beijing_air_plollution_2(final).ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1y-pdxy-P2Z8wSWKW-VVEv3k7QFrcbl_H",
      "authorship_tag": "ABX9TyPsjEUJAJKI0X+q3kf2z7no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YMGYM/TSE_Learning/blob/master/Beijing_air_plollution_2(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPythiWfQCnX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "이 파일은 황철현, 신강욱의\n",
        "`미세먼지 예측 성능 개선을 위한 CNN-LSTM 결합 방법`\n",
        "논문의 구현 연습 파일입니다.\n",
        "\n",
        "데이터셋은 [Beijing PM2.5 데이터셋](https://www.kaggle.com/djhavera/beijing-pm25-data-data-set)\n",
        "을 사용했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMnquIS4QfaO",
        "colab_type": "text"
      },
      "source": [
        "# Import All"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ9ZCMavQg_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGdslSrQXBM",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xY3TOYzQbgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d5464953-e982-4d99-f9ff-0c8de1d8fe89"
      },
      "source": [
        "! unzip /content/drive/My\\ Drive/Datasets/beijing_air.zip -d data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Datasets/beijing_air.zip\n",
            "  inflating: data/PRSA_data_2010.1.1-2014.12.31.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5W7oRhQ6Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "  all_data = pd.read_csv('/content/data/PRSA_data_2010.1.1-2014.12.31.csv') # 전체 데이터\n",
        "  dropped_data = all_data.drop(['No', 'year', 'month', 'day', 'hour'],axis=1) # 필요 없는 데이터는 버림\n",
        "  pm25 = dropped_data.pop('pm2.5') # 미세먼지 데이터 확인\n",
        "  pm25 = pm25.fillna(method='pad')\n",
        "  \n",
        "  return pm25, dropped_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5dnAMuOR1N0",
        "colab_type": "text"
      },
      "source": [
        "# Make Normalize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhbeIxNDR1hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PmScaler():\n",
        "  def __init__(self, pm25):\n",
        "    self.scaler = MinMaxScaler()\n",
        "    self.pm25 = pm25.fillna(1e-8, limit=1)\n",
        "    self.pm25 = self.pm25.fillna(method=\"pad\")\n",
        "    \n",
        "  def make_norlized_dataset(self, rate, x = None): \n",
        "    if x is None:\n",
        "      x = self.pm25\n",
        "    arrlen = int(len(x) * (rate))\n",
        "    if isinstance(x, type(np.array([]))) == False:\n",
        "      reshaped = x.to_numpy().reshape(-1,1)\n",
        "    else:\n",
        "      reshaped = x.reshape(-1,1)\n",
        "    scaled_data = self.scaler.fit_transform(reshaped)\n",
        "\n",
        "    train, val, test = scaled_data[:-1 * (arrlen * 2)], scaled_data[-1 * (arrlen * 2) : -1 * (arrlen)], scaled_data[-1 * (arrlen):]\n",
        "\n",
        "    return train, val, test\n",
        "  \n",
        "  def invert_scale(self, x):\n",
        "    inverse = self.scaler.inverse_transform(x)\n",
        "    return inverse"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1ktmAkpauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProxyDataScaler():\n",
        "  def __init__(self, data):\n",
        "    self.table = data\n",
        "    self.scaler = MinMaxScaler()\n",
        "    \n",
        "  def change_cbwd_data(self):\n",
        "    mapping = {}\n",
        "    cols = self.table[\"cbwd\"].value_counts().index\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "      mapping[col] = i # mapping = {\"SE\" : 0, \"NW\": 1, \"cv\": 2, \"NE\":3}\n",
        "    self.table = self.table.replace({'cbwd' : mapping})\n",
        "    print(\"cbwd data changed to number : {SE : 0, NW: 1, cv: 2, NE:3} \")\n",
        "\n",
        "  def make_normalize_data(self):\n",
        "    self.norm_data = self.scaler.fit_transform(self.table)\n",
        "    return self.norm_data\n",
        "\n",
        "  def slice_proxy_data(self, time):\n",
        "    col_cnt = len(self.table.columns)\n",
        "    if (self.norm_data is not None): # 정규화된 데이터가 있는지 확인\n",
        "      print(\"norm_data detected\")\n",
        "\n",
        "      if isinstance(self.norm_data, type(np.array([]))) == False: # 데이터를 numpy 형식으로 변환\n",
        "        data = self.norm_data.to_numpy().astype(\"float32\")\n",
        "      else:\n",
        "        data = self.norm_data.astype(\"float32\")\n",
        "\n",
        "    else:\n",
        "      print(\"norm_data not detected\")\n",
        "      data = self.table.to_numpy().astype(\"float32\")\n",
        "\n",
        "    self.sliced_data = np.zeros(shape=(1,time,col_cnt))\n",
        "\n",
        "    for i in range((len(data)-time) + 1):\n",
        "      if i == 0:\n",
        "        self.sliced_data = data[:i+time].reshape(1, time,-1)\n",
        "      else:\n",
        "        self.sliced_data = np.vstack((self.sliced_data, data[i:i+time].reshape(1,time,-1)))\n",
        "        \n",
        "    return self.sliced_data\n",
        "\n",
        "  def split_data(self, data = None, rate = 0.1):\n",
        "    if data is None:\n",
        "      arrlen = int(len(self.sliced_data) * (rate))\n",
        "      data = self.sliced_data\n",
        "    else:\n",
        "      arrlen = int(len(data) * rate)\n",
        "      data = data\n",
        "\n",
        "    train, val, test = data[:-1 * (arrlen * 2)], data[-1 * (arrlen * 2) : -1 * (arrlen)], data[-1 * (arrlen):]\n",
        "\n",
        "    return train, val, test"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COffyY3ow6By",
        "colab_type": "text"
      },
      "source": [
        "# New LSTM Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weEk_Ih-w89V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMInputGenerator(K.utils.Sequence):\n",
        "  def __init__(self, lstm_x, lstm_y, data_len, cnn_output, batch_size=1):\n",
        "    self.lstm_data_gen = K.preprocessing.sequence.TimeseriesGenerator(lstm_x, lstm_x, batch_size=batch_size, length=data_len, shuffle=False)\n",
        "    self.cnn_output = cnn_output\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    lstm_x, lstm_y = self.lstm_data_gen[index]\n",
        "    stack_data = self.cnn_output[index*self.batch_size : (index + 1)*self.batch_size].reshape(1,-1,1)\n",
        "    return_x = np.hstack((lstm_x, stack_data))\n",
        "    return return_x, lstm_y\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.cnn_output)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iO5V6OhRZNJ",
        "colab_type": "text"
      },
      "source": [
        "# Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arSPbMdqRepE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EntireModel():\n",
        "  def __init__(self, pm25, proxydata):\n",
        "    # -----------scaler 클래스 생성 -------------------\n",
        "    self.pm25 = pm25.fillna(1e-8, limit=1)\n",
        "    self.pm25 = self.pm25.fillna(method=\"pad\")\n",
        "    print(f\"pm25 length : {len(self.pm25)}\" )\n",
        "    self.proxydata = proxydata\n",
        "    print(f\"proxydata length : {len(self.proxydata)}\" )\n",
        "    self.pmScaler = PmScaler(pm25)\n",
        "    self.proxyScaler = ProxyDataScaler(proxydata)\n",
        "    \n",
        "    # ---------- train 용 callbacks -----------\n",
        "    self.lstm_callbacks = [K.callbacks.TensorBoard(log_dir='lstm_logs')]\n",
        "    self.cnn_callbacks = [K.callbacks.TensorBoard(log_dir='cnn_logs')]\n",
        "\n",
        "  def make_proxy_data_generator(self, data_len = 2):\n",
        "    # ---------- proxy 데이터 전처리 -----------\n",
        "    self.proxyScaler.change_cbwd_data() # 문자 데이터를 숫자 범주로\n",
        "    norm_data = self.proxyScaler.make_normalize_data() # 데이터 정규화\n",
        "    self.proxyScaler.slice_proxy_data(data_len) # 데이터 길이별로 자름\n",
        "    print(f\"sliced_data length : {len(self.proxyScaler.sliced_data)}// shape : {self.proxyScaler.sliced_data.shape} // data_len: {data_len}\" )\n",
        "    cnn_x_train, cnn_x_val, cnn_x_test = self.proxyScaler.split_data( rate = 0.1) # x 데이터 분할\n",
        "    print(f\"cnn_x_train : {len(cnn_x_train)}, {cnn_x_train.shape}, cnn_x_val : {len(cnn_x_val)}, cnn_x_test length: {len(cnn_x_test)}  // data_len: {data_len}\" )\n",
        "    self.grad_level = self._get_grad_pm25() # 미세먼지 변화량을 구함\n",
        "    print(f\"grad_level length : {len(self.grad_level)} // data_len: {data_len}\" )\n",
        "    cnn_y_train, cnn_y_val, cnn_y_test = self._cnn_y_split(self.grad_level, data_len) # y 데이터 분할\n",
        "    print(f\"cnn_y_train : {len(cnn_y_train)}, cnn_y_val : {len(cnn_y_val)}, cnn_x_test length: {len(cnn_y_test)}  // data_len: {data_len}\" )\n",
        "    # ---------- Data Generator 생성 ---------\n",
        "    self.cnn_train_data_gen = K.preprocessing.sequence.TimeseriesGenerator(cnn_x_train, cnn_y_train, length=1, batch_size = 128, shuffle=False)\n",
        "    self.cnn_val_data_gen = K.preprocessing.sequence.TimeseriesGenerator(cnn_x_val, cnn_y_val, length=1, batch_size = 128, shuffle=False)\n",
        "    self.cnn_test_data_gen = K.preprocessing.sequence.TimeseriesGenerator(cnn_x_test, cnn_y_test, length=1, batch_size = 1, shuffle=False)\n",
        "    print(f\"cnn_train_data_gen length : {self.cnn_train_data_gen.__len__()} // data_len: {data_len}\" )\n",
        "    # ----------- 모델 생성 --------------\n",
        "    print(\"Make CNN Model....\")\n",
        "    self.cnn_model = self._get_cnn_model(input_shape=(1, cnn_x_train.shape[1], cnn_x_train.shape[2]))\n",
        "\n",
        "  def make_lstm_data_generator(self, data_len = 15, batch_size = 1):\n",
        "    # ---------- pm25 데이터 전처리 -----------\n",
        "    pm25_x_train, pm25_x_val, pm25_x_test = self.pmScaler.make_norlized_dataset(rate = 0.1)\n",
        "    print(f\"pm25_x_train length : {len(pm25_x_train)}\" )\n",
        "    print(f\"pm25_x_val length : {len(pm25_x_val)}\" )\n",
        "    print(f\"pm25_x_test length : {len(pm25_x_test)}\" )\n",
        "\n",
        "    # ---------- pm25 Data Generator -------------\n",
        "    self.pm25_train_data_gen = K.preprocessing.sequence.TimeseriesGenerator(pm25_x_train, pm25_x_train, batch_size=batch_size, length=data_len, shuffle=False)\n",
        "    self.pm25_val_data_gen = K.preprocessing.sequence.TimeseriesGenerator(pm25_x_val, pm25_x_val, batch_size = batch_size, length=data_len, shuffle=False)\n",
        "    self.pm25_test_data_gen = K.preprocessing.sequence.TimeseriesGenerator(pm25_x_test, pm25_x_test, batch_size = 1, length=data_len, shuffle=False)\n",
        "\n",
        "    # ---------- CNN result ------------\n",
        "    print(\"get CNN output..\")\n",
        "    self.cnn_train_output = self.cnn_model.predict(self.cnn_train_data_gen)\n",
        "    cnn_train_output_gen = K.preprocessing.sequence.TimeseriesGenerator(self.cnn_train_output, self.cnn_train_output, batch_size = batch_size, length=1, shuffle=False)\n",
        "    rated_cnn_train_output = self._make_lstm_input(cnn_train_output_gen, self.pm25_train_data_gen)\n",
        "\n",
        "    self.cnn_val_output = self.cnn_model.predict(self.cnn_val_data_gen)\n",
        "    cnn_val_output_gen = K.preprocessing.sequence.TimeseriesGenerator(self.cnn_val_output, self.cnn_val_output, batch_size = batch_size, length=1, shuffle=False)\n",
        "    rated_cnn_val_output = self._make_lstm_input(cnn_val_output_gen, self.pm25_val_data_gen)\n",
        "\n",
        "    self.cnn_test_output = self.cnn_model.predict(self.cnn_test_data_gen)\n",
        "    cnn_test_output_gen = K.preprocessing.sequence.TimeseriesGenerator(self.cnn_test_output, self.cnn_test_output, batch_size = batch_size, length=1, shuffle=False)\n",
        "    rated_cnn_test_output = self._make_lstm_input(cnn_test_output_gen, self.pm25_test_data_gen)\n",
        "\n",
        "    # ---------- LSTM data gen ---------\n",
        "    self.lstm_train_data_gen = LSTMInputGenerator(pm25_x_train, pm25_x_train, data_len, rated_cnn_train_output, batch_size = batch_size)\n",
        "    self.lstm_val_data_gen = LSTMInputGenerator(pm25_x_val, pm25_x_val, data_len, rated_cnn_val_output, batch_size = 1)\n",
        "    self.lstm_test_data_gen = LSTMInputGenerator(pm25_x_test, pm25_x_test, data_len, rated_cnn_test_output, batch_size = 1)\n",
        "\n",
        "    # ---------- 모델 생성\n",
        "    print(\"make LSTM model...\")\n",
        "    self.lstm_model = self._get_lstm_model()\n",
        "\n",
        "  def cnn_model_fit(self, epochs=1):\n",
        "    self.cnn_model.fit(x=self.cnn_train_data_gen, epochs=epochs, validation_data=(self.cnn_val_data_gen), callbacks=self.cnn_callbacks)\n",
        "\n",
        "  def lstm_model_fit(self, epochs=1):\n",
        "    self.lstm_model.fit(x=self.lstm_train_data_gen, epochs=epochs, validation_data=self.lstm_val_data_gen, callbacks=self.lstm_callbacks)\n",
        "\n",
        "  def total_model_evaluate(self):\n",
        "    return self.lstm_model.evaluate(self.lstm_test_data_gen)\n",
        "\n",
        "  def _get_cnn_model(self, input_shape):\n",
        "    cnnModel = K.Sequential()\n",
        "    cnnModel.add(K.layers.Conv2DTranspose(32, (2,2), input_shape=input_shape, activation=\"relu\"))\n",
        "    cnnModel.add(K.layers.MaxPool2D(strides=2))\n",
        "    cnnModel.add(K.layers.Flatten())\n",
        "    cnnModel.add(K.layers.Dropout(0.1))\n",
        "    cnnModel.add(K.layers.Dense(100, activation=\"relu\"))\n",
        "    cnnModel.add(K.layers.ReLU())\n",
        "    cnnModel.add(K.layers.Dense(5, activation=\"softmax\"))\n",
        "    cnnModel.summary()\n",
        "    cnnModel.compile(optimizer=\"adam\", loss=\"MSE\")\n",
        "\n",
        "    return cnnModel\n",
        "\n",
        "  def _get_lstm_model(self):\n",
        "    lstm_model = K.Sequential()\n",
        "    lstm_model.add(K.layers.LSTM(216, input_shape=(16,1)))\n",
        "    lstm_model.add(K.layers.Dropout(0.3))\n",
        "    lstm_model.add(K.layers.Dense(128, activation=\"relu\"))\n",
        "    lstm_model.add(K.layers.Dropout(0.3))\n",
        "    lstm_model.add(K.layers.Dense(1, activation=\"sigmoid\"))\n",
        "    lstm_model.summary()\n",
        "    lstm_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "  def _cnn_y_split(self, data, data_length, rate=0.1):\n",
        "    # cnn에 입력할 Y 데이터를 나눔\n",
        "    data = data[data_length-1:]\n",
        "    data = K.utils.to_categorical(data)\n",
        "    arrlen = int(len(data) * rate)\n",
        "\n",
        "    train, val, test =  data[:-1 * (arrlen * 2)], data[-1 * (arrlen * 2) : -1 * (arrlen)], data[-1 * (arrlen):]\n",
        "    return train, val, test\n",
        "\n",
        "  def _get_grad_pm25(self):\n",
        "    # pm25 의 변화율을 구하고 범주화함\n",
        "    grad_data = self.pm25.pct_change()\n",
        "    grad_data = grad_data.fillna(method=\"pad\")\n",
        "    bins = [-9.166667e-02, -1e-15,1e-15, 1.212121e-01]\n",
        "    grad_level = np.digitize(grad_data, bins=bins, right=False)\n",
        "    return grad_level\n",
        "\n",
        "  def _get_rate(self, data):\n",
        "    # 변화율 별로 pm25의 예측량을 구해봄(임시)\n",
        "    index = data.argmax(axis=1)\n",
        "    rate = np.array([])\n",
        "    for i in range(len(index)):\n",
        "      if index[i] == 3:\n",
        "        rate = np.append(rate, 1)\n",
        "      else:\n",
        "        rate = np.append(rate, 1 + ((index[i]-3) * 0.25)) # 최대 50%의 변화율을 줘 봄\n",
        "    print(rate.shape)\n",
        "    return rate\n",
        "  \n",
        "  def _compute_with_data(self, data, value):\n",
        "    # 예측량을 구함\n",
        "    rate = self._get_rate(data)\n",
        "    return rate * value\n",
        "\n",
        "  def _make_lstm_input(self, cnn_output_gen, lstm_data_gen):\n",
        "    # 기존 lstm_input 에 위에서 구한 변화율을 곱한 뒤에 쌓음.\n",
        "    result = np.zeros(shape=(1,))\n",
        "    for i in range(cnn_output_gen.__len__()):\n",
        "      lx, ly = lstm_data_gen[i]\n",
        "      cx, cy = cnn_output_gen[i]\n",
        "      print(ly.shape)\n",
        "      if i == 0:\n",
        "        result = self._compute_with_data(cx.squeeze(axis=1), ly.squeeze(axis=1))\n",
        "        print(result.shape)\n",
        "      else:\n",
        "        result = np.hstack((result, self._compute_with_data(cx.squeeze(axis=1), ly.squeeze(axis=1))))\n",
        "    return result\n",
        "        "
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da1JnS-Q3rl-",
        "colab_type": "text"
      },
      "source": [
        "# Model Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUOca31BqV8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pm25, proxy = get_data()"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05e59Gkp4BwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "58c1b551-b0f7-4999-8fde-253222fe2cc8"
      },
      "source": [
        "model = EntireModel(pm25[:1000], proxy[:1000])"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pm25 length : 1000\n",
            "proxydata length : 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-eRVA1z4FPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "6bc380b9-5a5f-4143-8097-471be3a8e3c8"
      },
      "source": [
        "model.make_proxy_data_generator(data_len = 2)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cbwd data changed to number : {SE : 0, NW: 1, cv: 2, NE:3} \n",
            "norm_data detected\n",
            "sliced_data length : 999// shape : (999, 2, 7) // data_len: 2\n",
            "cnn_x_train : 801, (801, 2, 7), cnn_x_val : 99, cnn_x_test length: 99  // data_len: 2\n",
            "grad_level length : 1000 // data_len: 2\n",
            "cnn_y_train : 801, cnn_y_val : 99, cnn_x_test length: 99  // data_len: 2\n",
            "cnn_train_data_gen length : 7 // data_len: 2\n",
            "Make CNN Model....\n",
            "(801, 2, 7)\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_23 (Conv2DT (None, 2, 3, 32)          928       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 100)               3300      \n",
            "_________________________________________________________________\n",
            "re_lu_23 (ReLU)              (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 4,733\n",
            "Trainable params: 4,733\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEkiK5cS6OYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d06e01bf-6cc9-44c2-a5c0-ff9e12aae7e8"
      },
      "source": [
        "model.cnn_model_fit(epochs=1)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0429s). Check your callbacks.\n",
            "3/7 [===========>..................] - ETA: 0s - loss: 0.1599WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f4a450ba620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.1595 - val_loss: 0.1565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9OVQONU9Im3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "0fc70b2b-0b35-4d4f-a02c-6b0536d287ff"
      },
      "source": [
        "model.make_lstm_data_generator(data_len = 15, batch_size=128)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pm25_x_train length : 800\n",
            "pm25_x_val length : 100\n",
            "pm25_x_test length : 100\n",
            "get CNN output..\n",
            "(128, 1)\n",
            "(128,)\n",
            "(128,)\n",
            "(128, 1)\n",
            "(128,)\n",
            "(128, 1)\n",
            "(128,)\n",
            "(128, 1)\n",
            "(128,)\n",
            "(128, 1)\n",
            "(128,)\n",
            "(128, 1)\n",
            "(128,)\n",
            "(17, 1)\n",
            "(31,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-244-04509597d22c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lstm_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-239-850d499bdcf3>\u001b[0m in \u001b[0;36mmake_lstm_data_generator\u001b[0;34m(self, data_len, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_train_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_train_data_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcnn_train_output_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_train_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_train_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mrated_cnn_train_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_lstm_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_train_output_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpm25_train_data_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_val_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_val_data_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-239-850d499bdcf3>\u001b[0m in \u001b[0;36m_make_lstm_input\u001b[0;34m(self, cnn_output_gen, lstm_data_gen)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_with_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-239-850d499bdcf3>\u001b[0m in \u001b[0;36m_compute_with_data\u001b[0;34m(self, data, value)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# 예측량을 구함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_lstm_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_output_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_data_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (31,) (17,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwRdaKZiMc4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.lstm_model_fit(epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT-duD7y_N6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.total_model_evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyGpq_6NW_N_",
        "colab_type": "text"
      },
      "source": [
        "# 수정 필요 사항\n",
        "- 데이터를 100% 활용 하는게 맞는가?\n",
        "- 학습 속도 증가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWmnZ3MuQDAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}